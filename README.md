# Dream Come True: A Tale of Diffusion Model Tailored with CLIP and VLM.


<p align="center">
    <img src="https://github.com/user-attachments/assets/f91b3b5c-763c-4c06-8f15-ed6db96b8c9d"/>
</p>

## Dream Come True (DCT)
**DCT** is a framework for generating high-quality images from brain EEG signals.
This document introduces the precesedures required for replicating the results in *Dream Come True: A Tale of Diffusion Model Tailored with CLIP and VLM*.


## Architecture
![main model](https://github.com/user-attachments/assets/1e1f4a9a-b34e-41df-862d-15ae2bbe6399)

The **datasets** folder and **pretrains** folder are not included in this repository. 
Please download them from [eeg](https://github.com/perceivelab/eeg_visual_classification) and put them in the root directory of this repository as shown below. We also provide a copy of the Imagenet subset [imagenet](https://drive.google.com/file/d/1y7I9bG1zKYqBM94odcox_eQjnP9HGo9-/view?usp=drive_link).

For Stable Diffusion, we just use standard SD1.5. You can download it from the [official page of Stability](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main). You want the file ["v1-5-pruned.ckpt"](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main).

File path | Description
```

/pretrains
â”£ ðŸ“‚ models
â”ƒ   â”— ðŸ“œ config.yaml
â”ƒ   â”— ðŸ“œ v1-5-pruned.ckpt

â”£ ðŸ“‚ generation  
â”ƒ   â”— ðŸ“œ checkpoint_best.pth 

â”£ ðŸ“‚ eeg_pretain
â”ƒ   â”— ðŸ“œ checkpoint.pth  (pre-trained EEG encoder)

/datasets
â”£ ðŸ“‚ imageNet_images (subset of Imagenet)
â”£ ðŸ“œ imageNet_descriptions.txt

â”—  ðŸ“œ block_splits_by_image_all.pth
â”—  ðŸ“œ block_splits_by_image_single.pth 
â”—  ðŸ“œ eeg_5_95_std.pth  

/code
â”£ ðŸ“‚ sc_mbm
â”ƒ   â”— ðŸ“œ mae_for_eeg.py
â”ƒ   â”— ðŸ“œ trainer.py
â”ƒ   â”— ðŸ“œ utils.py

â”£ ðŸ“‚ dc_ldm
â”ƒ   â”— ðŸ“œ ldm_for_eeg.py
â”ƒ   â”— ðŸ“œ utils.py
â”ƒ   â”£ ðŸ“‚ models
â”ƒ   â”ƒ   â”— (adopted from LDM)
â”ƒ   â”£ ðŸ“‚ modules
â”ƒ   â”ƒ   â”— (adopted from LDM)

â”—  ðŸ“œ stageA1_eeg_pretrain.py   (main script for EEG pre-training)
â”—  ðŸ“œ eeg_ldm.py    (main script for fine-tuning stable diffusion)
â”—  ðŸ“œ gen_eval_eeg.py               (main script for generating images)

â”—  ðŸ“œ dataset.py                (functions for loading datasets)
â”—  ðŸ“œ eval_metrics.py           (functions for evaluation metrics)
â”—  ðŸ“œ config.py                 (configurations for the main scripts)

```


## Environment setup

Create and activate conda environment named ```dct``` from the ```env.yaml```
```sh
conda env create -f env.yaml
conda activate dct
```
The "imageNet_descriptions.txt" contains the pseudo descriptions of the images generated by VLM (Llava-NeXT). For more details on how to generate these pseudo descriptions, see Demo.ipynb.
You can directly generate pseudo descriptions via Llava-NeXT during finetuning if you have enough resources. In that case, no need for "imageNet_descriptions.txt" file.


## Pre-training on EEG data

You can download the dataset for pretraining from here [MOABB](https://github.com/NeuroTechX/moabb).

To perform the pre-training from scratch with defaults parameters, run 
```sh
python3 code/stageA1_eeg_pretrain.py
``` 

Hyper-parameters can be changed with command line arguments,
```sh
python3 code/stageA1_eeg_pretrain.py --mask_ratio 0.75 --num_epoch 800 --batch_size 2
```

Or the parameters can also be changed in ```code/config.py```

Multiple-GPU (DDP) training is supported, run with 
```sh
python -m torch.distributed.launch --nproc_per_node=NUM_GPUS code/stageA1_eeg_pretrain.py
```



## Finetune the Stable Diffusion with Pre-trained EEG Encoder
In this stage, the cross-attention heads and pre-trained EEG encoder will be jointly optimized with EEG-image pairs. 

```sh
python3 code/eeg_ldm.py --dataset EEG  --num_epoch 300 --batch_size 8 --pretrain_mbm_path ../pretrains/eeg_pretrain/checkpoint-eeg-500.pth
```


## Generating Images with Trained Checkpoints
Run this stage with our provided checkpoints: Here we provide a checkpoint [ckpt](https://drive.google.com/file/d/1Ygplxe1TB68-aYu082bjc89nD8Ngklnc/view?usp=drive_link), which you may want to try.
```sh
python3 code/gen_eval_eeg.py --dataset EEG --model_path  .\exps\results\generation\...\checkpoint.pth
```

![comparison_result](https://github.com/user-attachments/assets/b15412a5-7f4b-4da0-8a88-439ff9f6c2ce)

![gensamples](https://github.com/user-attachments/assets/075b824b-4787-44ae-858d-e01564467c60)


## Acknowledgement

This code is built upon the publicly available code [Mind-vis](https://github.com/zjc062/mind-vis),  [StableDiffusion](https://github.com/CompVis/stable-diffusion) and, [DreamDiffusion](https://github.com/bbaaii/DreamDiffusion). Thanks these authors for making their excellent work and codes publicly available.


## Citation ##
Please cite the following paper if you use this repository in your reseach.

```
Will be provided once published

